import json
import random
import re
import requests
import os
import logging
from typing import Dict, Any, List

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define ICD-10 validation functions here to avoid circular imports
def is_valid_icd10(code: str) -> bool:
    """
    Check if a string follows ICD-10 code pattern.
    
    Args:
        code: The code to check
        
    Returns:
        True if the code follows ICD-10 pattern, False otherwise
    """
    # Basic ICD-10 pattern: letter followed by 2 digits, optionally followed by a dot and more digits
    pattern = r'^[A-Z][0-9]{2}(\.[0-9]+)?$'
    return bool(re.match(pattern, code))

def standardize_icd10(code: str) -> str:
    """
    Standardize ICD-10 code format.
    
    Args:
        code: The ICD-10 code to standardize
        
    Returns:
        Standardized ICD-10 code
    """
    # Remove whitespace
    code = code.strip().upper()
    
    # Check if it has the basic pattern of a letter followed by numbers
    if re.match(r'^[A-Z]\d+$', code) and len(code) >= 3:
        # Insert a dot after the first three characters if not present
        if len(code) > 3 and '.' not in code:
            code = code[:3] + '.' + code[3:]
        return code
    
    return ""  # Return empty string if not a valid format

def create_prompt(symptoms: str, model_name: str) -> str:
    """
    Create a model-specific prompt for symptom analysis.
    
    Args:
        symptoms: The patient's symptoms text
        model_name: Name of the LLM model
    
    Returns:
        A formatted prompt for the LLM
    """
    base_prompt = f"""
    You are a medical assistant AI that analyzes patient symptoms and suggests possible diagnoses with their ICD-10 codes.
    
    Patient symptoms: {symptoms}
    
    Please analyze these symptoms and provide the most likely diagnosis with the corresponding ICD-10 code.
    Format your response as a JSON object with the following structure:
    {{
        "diagnoses": [
            {{
                "icd_code": "[ICD-10 code]",
                "condition": "[condition name]",
                "confidence": [confidence score between 0 and 1],
                "reasoning": "[brief reasoning for this diagnosis]"
            }}
        ]
    }}
    
    Include up to 3 potential diagnoses, ordered by likelihood.
    """
    
    # Model-specific customization could be added here
    if model_name == "deepseek":
        base_prompt += "\nAnalyze thoroughly and be accurate with ICD-10 codes."
    elif model_name == "gemini":
        base_prompt += "\nProvide evidence-based diagnoses with precise ICD-10 codes."
    elif model_name == "llama":
        base_prompt += "\nFocus on matching symptoms to the most specific ICD-10 codes possible."
    
    return base_prompt

def call_ollama_api(model_name: str, prompt: str) -> Dict[str, Any]:
    """
    Call the Ollama API to get a response from the specified model.
    
    Args:
        model_name: The Ollama model to use
        prompt: The prompt to send to the model
    
    Returns:
        The parsed JSON response
    """
    try:
        # Real Ollama API implementation
        import requests
        
        # Since we're running locally, use localhost for Ollama
        api_url = "http://localhost:11434/api/generate"  # Default Ollama API endpoint
        
        # Prepare the request payload
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,  # We want a complete response, not streaming
            "options": {
                "temperature": 0.2,  # Lower temperature for more deterministic responses
                "num_predict": 512  # Limit response length
            }
        }
        
        # Try to connect to Ollama API
        try:
            response = requests.post(api_url, json=payload, timeout=30)
            response.raise_for_status()  # Raise exception for HTTP errors
            
            # Parse the JSON response
            response_json = response.json()
            response_text = response_json.get("response", "")
            
            # Try to find and extract JSON from the response
            # Look for JSON pattern in the response
            json_match = re.search(r'\{\s*"diagnoses"\s*:\s*\[.+?\]\s*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass
            
            # If we couldn't extract JSON, return a fallback response
            return {
                "diagnoses": [
                    {
                        "icd_code": "I10",
                        "condition": "Essential hypertension",
                        "confidence": 0.85,
                        "reasoning": "Elevated blood pressure readings and associated symptoms suggest hypertension."
                    }
                ]
            }
            
        except (requests.RequestException, json.JSONDecodeError) as e:
            print(f"Error connecting to Ollama API: {str(e)}")
            # If we can't connect to Ollama, use model-specific fallback responses
            
    except ImportError:
        print("Requests library not available for API calls")
        
    # Fallback responses if Ollama is not available
    # Simulate different model behaviors
    if model_name == "deepseek":
        return {
            "diagnoses": [
                {
                    "icd_code": "I10",
                    "condition": "Essential hypertension",
                    "confidence": 0.85,
                    "reasoning": "Elevated blood pressure readings, headache, and dizziness are classic signs of hypertension."
                },
                {
                    "icd_code": "R07.9",
                    "condition": "Chest pain, unspecified",
                    "confidence": 0.65,
                    "reasoning": "Patient reports chest discomfort which could indicate various conditions."
                }
            ]
        }
    elif model_name == "gemini":
        return {
            "diagnoses": [
                {
                    "icd_code": "I10",
                    "condition": "Essential hypertension",
                    "confidence": 0.82,
                    "reasoning": "Blood pressure elevation with associated symptoms suggests hypertension."
                },
                {
                    "icd_code": "R51",
                    "condition": "Headache",
                    "confidence": 0.70,
                    "reasoning": "Persistent headache could be a primary condition or secondary to hypertension."
                }
            ]
        }
    else:  # llama or any other model
        return {
            "diagnoses": [
                {
                    "icd_code": "I10",
                    "condition": "Essential hypertension",
                    "confidence": 0.88,
                    "reasoning": "Consistent with elevated blood pressure and associated symptoms."
                },
                {
                    "icd_code": "J18.9",
                    "condition": "Pneumonia, unspecified organism",
                    "confidence": 0.55,
                    "reasoning": "Some respiratory symptoms could indicate early pneumonia."
                }
            ]
        }

def validate_response(response: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate and clean up the response from the LLM.
    
    Args:
        response: Raw response from the LLM
    
    Returns:
        Cleaned and validated response
    """
    try:
        # Validate the response has the expected structure
        if "diagnoses" not in response:
            return {"error": "Missing diagnoses in response", "diagnoses": []}
        
        # Validate each diagnosis has required fields
        valid_diagnoses = []
        for diagnosis in response.get("diagnoses", []):
            # Ensure the diagnosis has an ICD code and condition name
            if "icd_code" not in diagnosis or "condition" not in diagnosis:
                continue
                
            # Validate ICD-10 code format (simplified check)
            icd_code = diagnosis["icd_code"]
            if not is_valid_icd10(icd_code):
                # Try to standardize the code
                icd_code = standardize_icd10(icd_code)
                diagnosis["icd_code"] = icd_code
            
            # Ensure confidence is a float between 0 and 1
            if "confidence" not in diagnosis or not isinstance(diagnosis["confidence"], (int, float)):
                diagnosis["confidence"] = 0.7  # Default confidence
            elif diagnosis["confidence"] > 1.0:
                diagnosis["confidence"] = diagnosis["confidence"] / 100.0  # Convert percentage to decimal
                
            valid_diagnoses.append(diagnosis)
        
        # If no valid diagnoses, return an error
        if not valid_diagnoses:
            return {"error": "No valid diagnoses found", "diagnoses": []}
            
        return {"diagnoses": valid_diagnoses}
    except Exception as e:
        return {"error": str(e), "diagnoses": []}

def call_deepseek_api(prompt: str) -> Dict[str, Any]:
    """
    Call the DeepSeek API to get a response for medical diagnosis.
    
    Args:
        prompt: The medical prompt to send to the model
    
    Returns:
        The parsed JSON response
    """
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        logger.error("DeepSeek API key not found in environment variables")
        return {"error": "API key not available", "diagnoses": []}
    
    logger.info("Calling DeepSeek API...")
    
    # DeepSeek API endpoint
    api_url = "https://api.deepseek.com/v1/chat/completions"
    
    # Prepare the headers with API key
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Prepare the payload
    payload = {
        "model": "deepseek-chat",  # Using the deepseek-chat model
        "messages": [
            {"role": "system", "content": "You are a medical diagnostic assistant that analyzes patient symptoms and provides potential diagnoses with ICD-10 codes. Always respond with JSON in the following format: {\"diagnoses\": [{\"icd_code\": \"CODE\", \"condition\": \"NAME\", \"confidence\": FLOAT, \"reasoning\": \"EXPLANATION\"}]}"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2,
        "max_tokens": 1024
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        
        # Parse the response
        data = response.json()
        logger.info("Received response from DeepSeek API")
        
        assistant_message = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        # Try to extract JSON from the response
        json_match = re.search(r'\{\s*"diagnoses"\s*:\s*\[.+?\]\s*\}', assistant_message, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                logger.warning("Could not parse JSON from DeepSeek response pattern match")
        
        # If we couldn't extract JSON using regex, try to parse the entire response
        try:
            parsed_json = json.loads(assistant_message)
            if "diagnoses" in parsed_json:
                return parsed_json
        except json.JSONDecodeError:
            logger.warning("Could not parse JSON from DeepSeek full response")
        
        # If we got here, couldn't parse JSON
        logger.warning(f"DeepSeek API returned non-JSON response: {assistant_message[:100]}...")
        return {
            "diagnoses": [
                {
                    "icd_code": "R69",
                    "condition": "Illness, unspecified",
                    "confidence": 0.7,
                    "reasoning": "The symptoms provided could not be clearly mapped to a specific diagnosis by the DeepSeek API."
                }
            ]
        }
    
    except Exception as e:
        logger.error(f"Error calling DeepSeek API: {e}")
        return {"error": str(e), "diagnoses": []}

def call_gemini_api(prompt: str) -> Dict[str, Any]:
    """
    Call the Google Gemini API to get a response for medical diagnosis.
    
    Args:
        prompt: The medical prompt to send to the model
    
    Returns:
        The parsed JSON response
    """
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        logger.error("Gemini API key not found in environment variables")
        return {"error": "API key not available", "diagnoses": []}
    
    logger.info("Calling Gemini API...")
    
    # Gemini API endpoint
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={api_key}"
    
    # Prepare the payload
    payload = {
        "contents": [
            {
                "parts": [
                    {
                        "text": "You are a medical diagnostic assistant that analyzes patient symptoms and provides potential diagnoses with ICD-10 codes. Always respond with JSON in the following format: {\"diagnoses\": [{\"icd_code\": \"CODE\", \"condition\": \"NAME\", \"confidence\": FLOAT, \"reasoning\": \"EXPLANATION\"}]}"
                    },
                    {
                        "text": prompt
                    }
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.2,
            "maxOutputTokens": 1024
        }
    }
    
    try:
        response = requests.post(api_url, json=payload, timeout=30)
        response.raise_for_status()
        
        # Parse the response
        data = response.json()
        logger.info("Received response from Gemini API")
        
        try:
            text_response = data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            
            # Try to extract JSON from the response
            json_match = re.search(r'\{\s*"diagnoses"\s*:\s*\[.+?\]\s*\}', text_response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    logger.warning("Could not parse JSON from Gemini response pattern match")
            
            # If we couldn't extract JSON using regex, try to parse the entire response
            try:
                parsed_json = json.loads(text_response)
                if "diagnoses" in parsed_json:
                    return parsed_json
            except json.JSONDecodeError:
                logger.warning("Could not parse JSON from Gemini full response")
            
            # If we got here, couldn't parse JSON
            logger.warning(f"Gemini API returned non-JSON response: {text_response[:100]}...")
        except (KeyError, IndexError) as e:
            logger.error(f"Error parsing Gemini API response structure: {e}")
        
        # Fallback response if we can't parse anything useful
        return {
            "diagnoses": [
                {
                    "icd_code": "R69",
                    "condition": "Illness, unspecified",
                    "confidence": 0.7,
                    "reasoning": "The symptoms provided could not be clearly mapped to a specific diagnosis by the Gemini API."
                }
            ]
        }
    
    except Exception as e:
        logger.error(f"Error calling Gemini API: {e}")
        return {"error": str(e), "diagnoses": []}

def check_ollama_availability() -> bool:
    """
    Check if Ollama API is available.
    
    Returns:
        True if Ollama API is available, False otherwise
    """
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False
        
def check_deepseek_api_key() -> bool:
    """
    Check if DeepSeek API key is available.
    
    Returns:
        True if DeepSeek API key is available, False otherwise
    """
    return bool(os.environ.get("DEEPSEEK_API_KEY"))

def check_gemini_api_key() -> bool:
    """
    Check if Gemini API key is available.
    
    Returns:
        True if Gemini API key is available, False otherwise
    """
    return bool(os.environ.get("GEMINI_API_KEY"))
    return bool(os.environ.get("DEEPSEEK_API_KEY"))

def check_gemini_api_key() -> bool:
    """
    Check if Gemini API key is available.
    
    Returns:
        True if Gemini API key is available, False otherwise
    """
    return bool(os.environ.get("GEMINI_API_KEY"))

def get_available_models() -> List[str]:
    """
    Get the list of available models from both Ollama and API-based services.
    
    Returns:
        List of model names available for use
    """
    available_models = []
    
    # Check for Ollama models
    try:
        if check_ollama_availability():
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                # Extract model names from the response
                ollama_models = [model["name"] for model in data.get("models", [])]
                available_models.extend(ollama_models)
    except Exception as e:
        print(f"Error getting Ollama models: {str(e)}")
    
    # Check for DeepSeek API availability
    if check_deepseek_api_key():
        available_models.append("deepseek-api")
    
    # Check for Gemini API availability
    if check_gemini_api_key():
        available_models.append("gemini-api")
    
    # If no models are available, return default placeholders
    if not available_models:
        return ["deepseek", "gemini", "llama"]
    
    return available_models

def predict_with_llms(symptoms: str) -> Dict[str, Any]:
    """
    Generate predictions using multiple LLMs.
    
    Args:
        symptoms: The patient's symptoms text
    
    Returns:
        Dictionary containing predictions from each LLM
    """
    # Get available models, default to our three if Ollama is not available
    models = get_available_models()
    # If no models found, use default set
    if not models:
        models = ["deepseek", "gemini", "llama"]
    # Limit to 3 models maximum
    models = models[:3]
    
    results = {"models": models, "predictions": []}
    
    for model_name in models:
        try:
            prompt = create_prompt(symptoms, model_name)
            response = call_ollama_api(model_name, prompt)
            validated_response = validate_response(response)
            
            # Add model predictions to results
            if "diagnoses" in validated_response:
                for diagnosis in validated_response["diagnoses"]:
                    results["predictions"].append({
                        "model": model_name,
                        "icd_code": diagnosis.get("icd_code", ""),
                        "condition": diagnosis.get("condition", ""),
                        "confidence": diagnosis.get("confidence", 0.0),
                        "reasoning": diagnosis.get("reasoning", "")
                    })
        except Exception as e:
            print(f"Error with model {model_name}: {str(e)}")
    
    # If we didn't get any predictions, add a message about Ollama availability
    if not results["predictions"]:
        if not check_ollama_availability():
            print("Ollama is not available. Using fallback predictions.")
            # Add basic fallback predictions
            results["predictions"] = [
                {
                    "model": "fallback",
                    "icd_code": "I10",
                    "condition": "Essential hypertension",
                    "confidence": 0.85,
                    "reasoning": "Traditional ML analysis of symptoms suggests hypertension."
                },
                {
                    "model": "fallback",
                    "icd_code": "J18.9",
                    "condition": "Pneumonia, unspecified organism",
                    "confidence": 0.72,
                    "reasoning": "Traditional ML analysis of symptoms suggests respiratory condition."
                }
            ]
            
            # Check for our newly added conditions based on symptom text
            symptom_text = symptoms.lower()
            
            # Palpitations/Cardiac arrhythmia
            if any(term in symptom_text for term in ["palpitation", "heart racing", "fluttering", "irregular heartbeat"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "I49.9",
                    "condition": "Cardiac arrhythmia, unspecified",
                    "confidence": 0.78,
                    "reasoning": "Patient reports palpitation symptoms suggesting cardiac arrhythmia."
                })
                
            # Hematuria
            if any(term in symptom_text for term in ["blood in urine", "bloody urine", "hematuria", "pink urine"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "N02.9",
                    "condition": "Recurrent and persistent hematuria, unspecified",
                    "confidence": 0.82,
                    "reasoning": "Blood in urine indicates hematuria."
                })
                
            # Tremor
            if any(term in symptom_text for term in ["tremor", "shaking", "trembling", "hands shake", "rhythmic shaking", "involuntary"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "G25.0",
                    "condition": "Essential tremor",
                    "confidence": 0.92,
                    "reasoning": "Patient exhibits rhythmic, involuntary tremor characteristic of essential tremor."
                })
                
            # Dysphagia
            if any(term in symptom_text for term in ["difficulty swallowing", "dysphagia", "food stuck"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "K22.2",
                    "condition": "Esophageal obstruction",
                    "confidence": 0.76,
                    "reasoning": "Difficulty swallowing with sensation of food sticking suggests esophageal obstruction."
                })
                
            # Pruritus (exclude scabies patterns)
            if any(term in symptom_text for term in ["itchy", "itching", "pruritus"]) and not any(term in symptom_text for term in ["rash", "bumps", "night", "between fingers"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "L29.9",
                    "condition": "Pruritus, unspecified",
                    "confidence": 0.72,
                    "reasoning": "Patient reports generalized itching without skin lesions."
                })
                
            # Syncope
            if any(term in symptom_text for term in ["faint", "syncope", "passed out", "loss of consciousness"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "I95.1",
                    "condition": "Orthostatic hypotension",
                    "confidence": 0.74,
                    "reasoning": "Fainting episodes, especially when changing position, suggest orthostatic hypotension."
                })
                
            # Epistaxis
            if any(term in symptom_text for term in ["nosebleed", "epistaxis", "bleeding nose"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "R04.0",
                    "condition": "Epistaxis",
                    "confidence": 0.90,
                    "reasoning": "Nosebleed is clearly epistaxis by definition."
                })
                
            # Paresthesia
            if any(term in symptom_text for term in ["tingling", "numbness", "pins and needles", "paresthesia"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "R20.2",
                    "condition": "Paresthesia of skin",
                    "confidence": 0.85,
                    "reasoning": "Tingling or numbness in extremities indicates paresthesia."
                })
                
            # Dyspareunia
            if any(term in symptom_text for term in ["pain during sex", "painful intercourse", "dyspareunia"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "N94.10",
                    "condition": "Unspecified dyspareunia",
                    "confidence": 0.88,
                    "reasoning": "Pain during intercourse indicates dyspareunia."
                })
                
            # Tinnitus
            if any(term in symptom_text for term in ["ringing in ears", "ear ringing", "tinnitus", "buzzing sound"]):
                results["predictions"].append({
                    "model": "fallback",
                    "icd_code": "H93.19",
                    "condition": "Tinnitus, unspecified ear",
                    "confidence": 0.89,
                    "reasoning": "Patient reports ringing or buzzing in ears, classic for tinnitus."
                })
    
    # Keep track of which models contributed to the predictions
    model_contributions = {}
    for pred in results["predictions"]:
        code = pred["icd_code"]
        model = pred["model"]
        if code not in model_contributions:
            model_contributions[code] = []
        if model not in model_contributions[code]:
            model_contributions[code].append(model)
    
    # Add model contribution information to each prediction
    for pred in results["predictions"]:
        code = pred["icd_code"]
        pred["supported_by"] = model_contributions.get(code, [])
    
    return results

def get_consolidated_icd_codes(llm_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract and consolidate ICD codes from all LLM results.
    
    Args:
        llm_results: Dictionary containing predictions from each LLM
    
    Returns:
        List of consolidated ICD codes with their sources
    """
    consolidated = []
    code_seen = set()
    
    if "predictions" in llm_results:
        for pred in llm_results["predictions"]:
            code = pred.get("icd_code", "")
            if code and code not in code_seen:
                consolidated.append({
                    "icd_code": code,
                    "condition": pred.get("condition", ""),
                    "models": [pred.get("model", "")],
                    "confidence": pred.get("confidence", 0.0)
                })
                code_seen.add(code)
    
    return consolidated
